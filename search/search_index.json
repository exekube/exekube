{
    "docs": [
        {
            "location": "/", 
            "text": "Exekube documentation\n\n\nExekube is an \"Infrastructure as Code\" modular framework for managing the whole lifecycle of Kubernetes-based projects. Exekube is built with Terraform, Terragrunt, and Helm as its developer interfaces.\n\n\n\n\nNote\n\n\nDocumentation is for Exekube version \n0.1.0\n.\n\n\nCheck all Exekube releases: \nhttps://github.com/exekube/exekube/releases\n\n\n\n\nIntroduction\n\n\n\n\nWhat is Exekube?\n\n\nHow does Exekube compare to other software?\n\n\n\n\nSetup and Installation\n\n\n\n\nCreate an Exekube project on Google Cloud Platform\n\n\nCreate an Exekube project on Amazon Web Services\n\n\n\n\nUsage\n\n\n\n\nGuide to project directory structure and framework usage\n\n\nTutorial: deploy an application on Kubernetes with Exekube\n\n\n\n\nReference\n\n\n\n\ngke-cluster module\n\n\nhelm-release module\n\n\n\n\nMiscellaneous\n\n\n\n\nCompare using Helm CLI and terraform-provider-helm\n\n\nHow to configure a Helm release\n\n\nUse HashiCorp Vault to manage secrets\n\n\nRead the project's feature tracker\n\n\nManaging secrets in Exekube", 
            "title": "Overview"
        }, 
        {
            "location": "/#exekube-documentation", 
            "text": "Exekube is an \"Infrastructure as Code\" modular framework for managing the whole lifecycle of Kubernetes-based projects. Exekube is built with Terraform, Terragrunt, and Helm as its developer interfaces.   Note  Documentation is for Exekube version  0.1.0 .  Check all Exekube releases:  https://github.com/exekube/exekube/releases", 
            "title": "Exekube documentation"
        }, 
        {
            "location": "/#introduction", 
            "text": "What is Exekube?  How does Exekube compare to other software?", 
            "title": "Introduction"
        }, 
        {
            "location": "/#setup-and-installation", 
            "text": "Create an Exekube project on Google Cloud Platform  Create an Exekube project on Amazon Web Services", 
            "title": "Setup and Installation"
        }, 
        {
            "location": "/#usage", 
            "text": "Guide to project directory structure and framework usage  Tutorial: deploy an application on Kubernetes with Exekube", 
            "title": "Usage"
        }, 
        {
            "location": "/#reference", 
            "text": "gke-cluster module  helm-release module", 
            "title": "Reference"
        }, 
        {
            "location": "/#miscellaneous", 
            "text": "Compare using Helm CLI and terraform-provider-helm  How to configure a Helm release  Use HashiCorp Vault to manage secrets  Read the project's feature tracker  Managing secrets in Exekube", 
            "title": "Miscellaneous"
        }, 
        {
            "location": "/introduction/what-is-exekube/", 
            "text": "What is Exekube?\n\n\nExekube is an \"Infrastructure as Code\" modular framework for managing the whole lifecycle of Kubernetes-based projects. Exekube is built with Terraform, Terragrunt, and Helm as its developer interfaces.\n\n\nMotivation\n\n\nUsing many command line tools and GUIs to manage cloud resources (\ngcloud\n, \naws\n, \nkops\n) and Kubernetes resources (\nkubectl\n, \nhelm\n) is quite tedious and can be error-prone.\n\n\nTerraform is a very flexible declarative tool with support for a \nlarge number\n of cloud providers and can replace all of the said command line tools.\n\n\nExekube takes advantage of Terraform's power to give us \"sane default\" state for managing everything related to Kubernetes \nas declarative code\n in a fully automated, modular, git-based workflow.\n\n\nSample workflow\n\n\n\n\nTL;DR\n\n\nSpin up and then destroy a Kubernetes cluster and all Kubernetes resources:\n\nxk up \n xk down\n\n\n\n\n\n\n\n\n\nInitial setup\n: Create an empty project for your deployment environment on a cloud platform like Amazon Web Services (AWS) or Google Cloud Platform (GCP) and get credentials for it. This is only done once for every deployment environment.\n\n\nTutorial for Google Cloud Platform\n\n\n\n\n\n\nEdit code\n: Configure your project by editing Terraform (HCL) files in a text editor of your choice.\n\n\nGuide to Exekube directory structure and framework usage\n \u25cf \nExample project\n\n\n\n\n\n\nCreate a project\n: Run \nxk up\n to deploy everything onto the cloud platform, including cloud infrastructure and Kubernetes resources.\n\n\n\n\nUpdate the project\n: Edit Terraform code in face of changing requirements and run \nxk up\n again to match the state of your code to the state of real-world resources.\n\n\nDestroy the project\n: Run \nxk down\n to clean everything up.\n\n\n\n\nThis workflow is an excellent fit for creating easy-to-understand continuous integration pipelines.\n\n\nFeatures\n\n\nThe framework offers you:\n\n\n\n\nDocker-based cloud development environment with all necessary framework tools\n\n\nFull control over your cloud infrastructure (via Terraform)\n\n\nFull control over your container orchestration (via Terraform + Helm)\n\n\nFully automated one-command-to-deploy \nxk up\n and \nxk down\n experience\n\n\nModular design and declarative model of management\n\n\nFreedom to choose a cloud provider to host Kubernetes\n\n\nContinuous integration (CI) facilities out of the box\n\n\n\n\nComponents\n\n\nThe framework is distributed as a \nDocker image on DockerHub\n that can be used manually by DevOps engineers or automatically via continuous integration (CI) pipelines. It combines several open-source DevOps tools into one easy-to-use workflow for managing cloud infrastructure and Kubernetes resources.\n\n\nDevOps tools\n\n\n\n\n\n\n\n\nComponent\n\n\nRole\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nLocal and cloud container runtime\n\n\n\n\n\n\nDocker Compose\n\n\nLocal development enviroment manager\n\n\n\n\n\n\nTerraform\n\n\nDeclarative cloud infrastructure manager\n\n\n\n\n\n\nTerragrunt\n\n\nTerraform \nlive module\n manager\n\n\n\n\n\n\nKubernetes\n\n\nContainer orchestrator\n\n\n\n\n\n\nHelm\n\n\nKubernetes package (chart / release) manager\n\n\n\n\n\n\n\n\nDefault Helm packages installed\n\n\n\n\n\n\n\n\nComponent\n\n\nRole\n\n\n\n\n\n\n\n\n\n\nNGINX Ingress Controller\n\n\nCluster ingress controller\n\n\n\n\n\n\nkube-lego\n\n\nAutomatic Let's Encrypt TLS certificates for Ingress\n\n\n\n\n\n\nHashiCorp Vault (TBD)\n\n\nCluster secret management\n\n\n\n\n\n\nDocker Registry\n\n\nContainer image registry\n\n\n\n\n\n\nChartMuseum\n\n\nHelm chart repository\n\n\n\n\n\n\nJenkins, Drone, or Concourse\n\n\nContinuous integration", 
            "title": "What is Exekube?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#what-is-exekube", 
            "text": "Exekube is an \"Infrastructure as Code\" modular framework for managing the whole lifecycle of Kubernetes-based projects. Exekube is built with Terraform, Terragrunt, and Helm as its developer interfaces.", 
            "title": "What is Exekube?"
        }, 
        {
            "location": "/introduction/what-is-exekube/#motivation", 
            "text": "Using many command line tools and GUIs to manage cloud resources ( gcloud ,  aws ,  kops ) and Kubernetes resources ( kubectl ,  helm ) is quite tedious and can be error-prone.  Terraform is a very flexible declarative tool with support for a  large number  of cloud providers and can replace all of the said command line tools.  Exekube takes advantage of Terraform's power to give us \"sane default\" state for managing everything related to Kubernetes  as declarative code  in a fully automated, modular, git-based workflow.", 
            "title": "Motivation"
        }, 
        {
            "location": "/introduction/what-is-exekube/#sample-workflow", 
            "text": "TL;DR  Spin up and then destroy a Kubernetes cluster and all Kubernetes resources: xk up   xk down     Initial setup : Create an empty project for your deployment environment on a cloud platform like Amazon Web Services (AWS) or Google Cloud Platform (GCP) and get credentials for it. This is only done once for every deployment environment.  Tutorial for Google Cloud Platform    Edit code : Configure your project by editing Terraform (HCL) files in a text editor of your choice.  Guide to Exekube directory structure and framework usage  \u25cf  Example project    Create a project : Run  xk up  to deploy everything onto the cloud platform, including cloud infrastructure and Kubernetes resources.   Update the project : Edit Terraform code in face of changing requirements and run  xk up  again to match the state of your code to the state of real-world resources.  Destroy the project : Run  xk down  to clean everything up.   This workflow is an excellent fit for creating easy-to-understand continuous integration pipelines.", 
            "title": "Sample workflow"
        }, 
        {
            "location": "/introduction/what-is-exekube/#features", 
            "text": "The framework offers you:   Docker-based cloud development environment with all necessary framework tools  Full control over your cloud infrastructure (via Terraform)  Full control over your container orchestration (via Terraform + Helm)  Fully automated one-command-to-deploy  xk up  and  xk down  experience  Modular design and declarative model of management  Freedom to choose a cloud provider to host Kubernetes  Continuous integration (CI) facilities out of the box", 
            "title": "Features"
        }, 
        {
            "location": "/introduction/what-is-exekube/#components", 
            "text": "The framework is distributed as a  Docker image on DockerHub  that can be used manually by DevOps engineers or automatically via continuous integration (CI) pipelines. It combines several open-source DevOps tools into one easy-to-use workflow for managing cloud infrastructure and Kubernetes resources.", 
            "title": "Components"
        }, 
        {
            "location": "/introduction/what-is-exekube/#devops-tools", 
            "text": "Component  Role      Docker  Local and cloud container runtime    Docker Compose  Local development enviroment manager    Terraform  Declarative cloud infrastructure manager    Terragrunt  Terraform  live module  manager    Kubernetes  Container orchestrator    Helm  Kubernetes package (chart / release) manager", 
            "title": "DevOps tools"
        }, 
        {
            "location": "/introduction/what-is-exekube/#default-helm-packages-installed", 
            "text": "Component  Role      NGINX Ingress Controller  Cluster ingress controller    kube-lego  Automatic Let's Encrypt TLS certificates for Ingress    HashiCorp Vault (TBD)  Cluster secret management    Docker Registry  Container image registry    ChartMuseum  Helm chart repository    Jenkins, Drone, or Concourse  Continuous integration", 
            "title": "Default Helm packages installed"
        }, 
        {
            "location": "/introduction/exekube-vs-other/", 
            "text": "Compare Exekube to other software\n\n\n\n\nWarning\n\n\nThis article is incomplete. Want to help? \nSubmit a pull request\n.\n\n\n\n\nvs CLI tools / shell scripts\n\n\nCLI tools / Exekube legacy imperative workflow\n\n\nCommand line tools \nkubectl\n and \nhelm\n are known to those who are familiar with Kubernetes. \ngcloud\n (part of Google Cloud SDK) is used for managing the Google Cloud Platform.\n\n\n\n\ngcloud \ngroup\n \ncommand\n \narguments\n \nflags\n\n\nkubectl \ngroup\n \ncommand\n \narguments\n \nflags\n\n\nhelm \ncommand\n \narguments\n \nflags\n\n\n\n\nExamples:\n\n\ngcloud auth list\n\nkubectl get nodes\n\nhelm install --name custom-rails-app \n\\\n\n        -f live/prod/kube/apps/my-app/values.yaml \n\\\n\n        charts/rails-app\n\n\n\n\nDeclarative workflow\n\n\n\n\nxk up\n\n\nxk down\n\n\n\n\nDeclarative tools are exact equivalents of stadard CLI tools like \ngcloud\n / \naws\n, \nkubectl\n, and \nhelm\n, except everything is implemented as a \nTerraform provider plugin\n and expressed as declarative HCL (HashiCorp Configuration Language) code.", 
            "title": "Exekube in comparison"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#compare-exekube-to-other-software", 
            "text": "Warning  This article is incomplete. Want to help?  Submit a pull request .", 
            "title": "Compare Exekube to other software"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#vs-cli-tools-shell-scripts", 
            "text": "", 
            "title": "vs CLI tools / shell scripts"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#cli-tools-exekube-legacy-imperative-workflow", 
            "text": "Command line tools  kubectl  and  helm  are known to those who are familiar with Kubernetes.  gcloud  (part of Google Cloud SDK) is used for managing the Google Cloud Platform.   gcloud  group   command   arguments   flags  kubectl  group   command   arguments   flags  helm  command   arguments   flags   Examples:  gcloud auth list\n\nkubectl get nodes\n\nhelm install --name custom-rails-app  \\ \n        -f live/prod/kube/apps/my-app/values.yaml  \\ \n        charts/rails-app", 
            "title": "CLI tools / Exekube legacy imperative workflow"
        }, 
        {
            "location": "/introduction/exekube-vs-other/#declarative-workflow", 
            "text": "xk up  xk down   Declarative tools are exact equivalents of stadard CLI tools like  gcloud  /  aws ,  kubectl , and  helm , except everything is implemented as a  Terraform provider plugin  and expressed as declarative HCL (HashiCorp Configuration Language) code.", 
            "title": "Declarative workflow"
        }, 
        {
            "location": "/setup/gcp-gke/", 
            "text": "Setup a project space on Google Cloud Platform\n\n\nRequirements starting from zero\n\n\n\n\nFor Linux users, \nDocker CE\n and \nDocker Compose\n are sufficient\n\n\nFor macOS users, \nDocker for Mac\n is sufficient\n\n\nFor Windows users, \nDocker for Windows\n is sufficient\n\n\n\n\nStep-by-step instructions\n\n\n\n\n\n\nClone the example project (\ninternal-ops-project\n) git repo:\n\n\ngit clone https://github.com/exekube/internal-ops-project \n\\\n\n\n \ncd\n exekube\n\n\n\n\n\n\n\n\nCreate a bash alias for your shell session (\nxk\n stands for \"exekube\"):\n\n\nalias\n \nxk\n=\ndocker-compose run --rm exekube\n\n\n\n\n\n\n\n\n\nIf you don't already have one, create a \nGoogle Account\n. Then, create a new \nGCP Project\n.\n\n\n\n\n\n\n\n\nProject name\n\n\nProject ID\n\n\n\n\n\n\n\n\n\n\nProduction Project\n\n\nproduction-project-20180101\n\n\n\n\n\n\n\n\n\n\n\n\nSet the variables for your environment in \nlive/prod/.env\n:\n\n\nGOOGLE_CREDENTIALS\n=\n/project/live/prod/secrets/sa-key.json\n\nTF_VAR_xk_live_dir\n=\n/project/live/prod\n\nTF_VAR_gcp_project\n=\nproduction-project-20180101\n\nTF_VAR_gcp_remote_state_bucket\n=\nproduction-project-20180101-tfstate\n\n\n\n\n\n\n\n\nCreate a service account\n and give it project owner permissions. A JSON-econded private key file will be downloaded onto your machine, which you'll need to move into \nlive/prod/secrets/\n directory and rename to \nsa-key.json\n.\n\n\n\n\n\n\n\n\nFinally, use the key to authenticate to the Google Cloud SDK and create a Google Cloud Storage bucket (with versioning) for our Terraform remote state:\n\n\nchmod \n600\n live/prod/secrets/sa-key.json \n\\\n\n\n xk gcloud auth activate-service-account \n\\\n\n        --key-file live/prod/secrets/sa-key.json \n\\\n\n\n xk gsutil mb \n\\\n\n        -p \n$TF_VAR_gcp_project\n \n\\\n\n        gs://\n$TF_VAR_gcp_remote_state_bucket\n \n\\\n\n\n xk gsutil versioning \nset\n on \n\\\n\n        gs://\n$TF_VAR_gcp_remote_state_bucket\n\n\n\n\n\n\n\n\n\n\u2705 You project space on the Google Cloud Platform is now ready!\n\n\nUp next\n\n\n\n\nTutorial: deploy an application on Kubernetes with Exekube\n\n\nGuide to Exekube directory structure and framework usage", 
            "title": "On Google Cloud Platform"
        }, 
        {
            "location": "/setup/gcp-gke/#setup-a-project-space-on-google-cloud-platform", 
            "text": "", 
            "title": "Setup a project space on Google Cloud Platform"
        }, 
        {
            "location": "/setup/gcp-gke/#requirements-starting-from-zero", 
            "text": "For Linux users,  Docker CE  and  Docker Compose  are sufficient  For macOS users,  Docker for Mac  is sufficient  For Windows users,  Docker for Windows  is sufficient", 
            "title": "Requirements starting from zero"
        }, 
        {
            "location": "/setup/gcp-gke/#step-by-step-instructions", 
            "text": "Clone the example project ( internal-ops-project ) git repo:  git clone https://github.com/exekube/internal-ops-project  \\    cd  exekube    Create a bash alias for your shell session ( xk  stands for \"exekube\"):  alias   xk = docker-compose run --rm exekube     If you don't already have one, create a  Google Account . Then, create a new  GCP Project .     Project name  Project ID      Production Project  production-project-20180101       Set the variables for your environment in  live/prod/.env :  GOOGLE_CREDENTIALS = /project/live/prod/secrets/sa-key.json TF_VAR_xk_live_dir = /project/live/prod TF_VAR_gcp_project = production-project-20180101 TF_VAR_gcp_remote_state_bucket = production-project-20180101-tfstate    Create a service account  and give it project owner permissions. A JSON-econded private key file will be downloaded onto your machine, which you'll need to move into  live/prod/secrets/  directory and rename to  sa-key.json .     Finally, use the key to authenticate to the Google Cloud SDK and create a Google Cloud Storage bucket (with versioning) for our Terraform remote state:  chmod  600  live/prod/secrets/sa-key.json  \\   xk gcloud auth activate-service-account  \\ \n        --key-file live/prod/secrets/sa-key.json  \\   xk gsutil mb  \\ \n        -p  $TF_VAR_gcp_project   \\ \n        gs:// $TF_VAR_gcp_remote_state_bucket   \\   xk gsutil versioning  set  on  \\ \n        gs:// $TF_VAR_gcp_remote_state_bucket     \u2705 You project space on the Google Cloud Platform is now ready!", 
            "title": "Step-by-step instructions"
        }, 
        {
            "location": "/setup/gcp-gke/#up-next", 
            "text": "Tutorial: deploy an application on Kubernetes with Exekube  Guide to Exekube directory structure and framework usage", 
            "title": "Up next"
        }, 
        {
            "location": "/setup/aws-eks/", 
            "text": "Setup Exekube with Amazon Web Services\n\n\nUse kops\n\n\n\n\nMissing\n\n\nThis section has not been written yet. Want to help? \nSubmit a pull request\n.\n\n\n\n\nUse EKS (Elastic Container Service for Kubernetes)\n\n\n\n\nMissing\n\n\nThis section has not been written yet. Want to help? \nSubmit a pull request\n.", 
            "title": "On Amazon Web Services"
        }, 
        {
            "location": "/setup/aws-eks/#setup-exekube-with-amazon-web-services", 
            "text": "", 
            "title": "Setup Exekube with Amazon Web Services"
        }, 
        {
            "location": "/setup/aws-eks/#use-kops", 
            "text": "Missing  This section has not been written yet. Want to help?  Submit a pull request .", 
            "title": "Use kops"
        }, 
        {
            "location": "/setup/aws-eks/#use-eks-elastic-container-service-for-kubernetes", 
            "text": "Missing  This section has not been written yet. Want to help?  Submit a pull request .", 
            "title": "Use EKS (Elastic Container Service for Kubernetes)"
        }, 
        {
            "location": "/usage/deploy-app/", 
            "text": "Deploy an application on Kubernetes with Exekube\n\n\n\n\nWarning\n\n\nThis article is incomplete. Want to help? \nSubmit a pull request\n.\n\n\n\n\n\n\n\n\nEdit code in \nlive\n:\n\n\nGuide to Terraform / Terragrunt, HCL, and Exekube directory structure\n\n\n\n\n\n\nApply all \nTerraform live modules\n \u2014 create all cloud infrastructure and all Kubernetes resources:\n\n\nxk up\n\n+ ...\n\n\n+ Module /exekube/live/prod/kube/apps/rails-app has finished successfully!\n\n\n\n\n\n\n\n\n\nEnable the Kubernetes dashboard at \nhttp://localhost:8001/ui\n:\n\n\ndocker-compose up -d\n\n\n\n\n\n\n\n\nGo to \nhttps://my-app.YOURDOMAIN.COM/\n to check that a hello-world Rails app is running.\n\n\n\n\n\n\nUpgrade the Rails application Docker image version in \nlive/kube/apps/my-app/values.yaml\n:\n\n\n replicaCount: 2\n image:\n   repository: ilyasotkov/rails-react-boilerplate\n\n-  tag: \n1.0.0\n\n\n+  tag: \n1.0.1\n\n   pullPolicy: Always\n\n\n\n\nUpgrade the state of real-world cloud resources to the state of our code in \nlive/prod\n directory:\n\nxk up\n\n\nGo back to your browser and check how your app updated with zero downtime! \ud83d\ude0e\n\n\n\n\n\n\nExperiment with creating, upgrading, and destroying single live modules and groups of live modules:\n\n\nxk down live/prod/releases/rails-app/\nxk down live/prod/kube/apps/\n\nxk up live/prod/kube/\nxk up live/prod/kube/apps/rails-app/\n\n\n\n\n\n\n\n\nClean everything up:\n\n\n# Destroy all cloud provider and Kubernetes resources\n\nxk down", 
            "title": "Deploy an application"
        }, 
        {
            "location": "/usage/deploy-app/#deploy-an-application-on-kubernetes-with-exekube", 
            "text": "Warning  This article is incomplete. Want to help?  Submit a pull request .     Edit code in  live :  Guide to Terraform / Terragrunt, HCL, and Exekube directory structure    Apply all  Terraform live modules  \u2014 create all cloud infrastructure and all Kubernetes resources:  xk up + ...  + Module /exekube/live/prod/kube/apps/rails-app has finished successfully!     Enable the Kubernetes dashboard at  http://localhost:8001/ui :  docker-compose up -d    Go to  https://my-app.YOURDOMAIN.COM/  to check that a hello-world Rails app is running.    Upgrade the Rails application Docker image version in  live/kube/apps/my-app/values.yaml :   replicaCount: 2\n image:\n   repository: ilyasotkov/rails-react-boilerplate -  tag:  1.0.0  +  tag:  1.0.1 \n   pullPolicy: Always  Upgrade the state of real-world cloud resources to the state of our code in  live/prod  directory: xk up \nGo back to your browser and check how your app updated with zero downtime! \ud83d\ude0e    Experiment with creating, upgrading, and destroying single live modules and groups of live modules:  xk down live/prod/releases/rails-app/\nxk down live/prod/kube/apps/\n\nxk up live/prod/kube/\nxk up live/prod/kube/apps/rails-app/    Clean everything up:  # Destroy all cloud provider and Kubernetes resources \nxk down", 
            "title": "Deploy an application on Kubernetes with Exekube"
        }, 
        {
            "location": "/usage/directory-structure/", 
            "text": "Guide to Exekube directory structure and framework usage\n\n\n\n\nTip\n\n\nCheck out the \ninternal-ops-project\n to see an example directory structure of a cloud project managed by Exekube.\n\n\n\n\nGeneric modules\n\n\nGeneric modules are normal Terraform modules, just like the ones you can find at \nhttps://modules.terraform.io\n.\n\n\nGeneric modules are \nsame across different deployment environments\n.\n\n\nGeneric modules are imported by \nlive modules\n via Terragrunt like that:\n\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    # Import a generic module from the local filesystem\n\n\n    source\n \n=\n \n/exekube-modules//gke-cluster\n\n  \n}\n\n\n  # ...\n\n\n}\n\n\n\nor like that:\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    # Import a generic module from a remote git repo\n\n\n    source\n \n=\n \ngit::git@github.com:foo/modules.git//app?ref\n=\nv\n0\n.\n0\n.\n3\n\n  \n}\n\n\n  # ...\n\n\n}\n\n\n\n\nCurrently, Exekube ships with two built-in generic modules:\n\n\n\n\ngke-cluster\n module, which can create a Kubernetes cluster and an auto-scaling node pool on Google Kubernetes Engine\n\n\nhelm-release\n module, which can deploy (release) a Helm chart onto a Kubernetes cluster\n\n\n\n\nLive modules\n\n\nLive modules are applicable / executable modules, the modules that will be located in the \nlive\n directory and applied by Terraform. Exekube uses Terragrunt as a wrapper around Terraform to to reduce boilerplate code for live modules and manage multiple live modules at once.\n\n\nLive modules are instances of generic modules configured for a specific deployment environment. Live modules are always \ndifferent across different deployment environments\n.\n\n\nIf you run \nxk up\n, you are applying \nall live modules\n, so it is equivalent of running \nxk up $TF_VAR_xk_live_dir\n. Under the cover, \nxk up\n calls \nterragrunt apply-all\n.\n\n\nYou can also apply an individual live module by running \nxk up \nlive-module-path\n or groups of live modules by running \nxk up \ndirectory-structure-of-live-modules\n.", 
            "title": "Guide to Exekube directory structure and framework usage"
        }, 
        {
            "location": "/usage/directory-structure/#guide-to-exekube-directory-structure-and-framework-usage", 
            "text": "Tip  Check out the  internal-ops-project  to see an example directory structure of a cloud project managed by Exekube.", 
            "title": "Guide to Exekube directory structure and framework usage"
        }, 
        {
            "location": "/usage/directory-structure/#generic-modules", 
            "text": "Generic modules are normal Terraform modules, just like the ones you can find at  https://modules.terraform.io .  Generic modules are  same across different deployment environments .  Generic modules are imported by  live modules  via Terragrunt like that:  terragrunt   =   { \n   terraform   {      # Import a generic module from the local filesystem      source   =   /exekube-modules//gke-cluster \n   }    # ...  }  \nor like that: terragrunt   =   { \n   terraform   {      # Import a generic module from a remote git repo      source   =   git::git@github.com:foo/modules.git//app?ref = v 0 . 0 . 3 \n   }    # ...  }   Currently, Exekube ships with two built-in generic modules:   gke-cluster  module, which can create a Kubernetes cluster and an auto-scaling node pool on Google Kubernetes Engine  helm-release  module, which can deploy (release) a Helm chart onto a Kubernetes cluster", 
            "title": "Generic modules"
        }, 
        {
            "location": "/usage/directory-structure/#live-modules", 
            "text": "Live modules are applicable / executable modules, the modules that will be located in the  live  directory and applied by Terraform. Exekube uses Terragrunt as a wrapper around Terraform to to reduce boilerplate code for live modules and manage multiple live modules at once.  Live modules are instances of generic modules configured for a specific deployment environment. Live modules are always  different across different deployment environments .  If you run  xk up , you are applying  all live modules , so it is equivalent of running  xk up $TF_VAR_xk_live_dir . Under the cover,  xk up  calls  terragrunt apply-all .  You can also apply an individual live module by running  xk up  live-module-path  or groups of live modules by running  xk up  directory-structure-of-live-modules .", 
            "title": "Live modules"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/", 
            "text": "Compare deploying Helm releases via Helm CLI to using terraform-provider-helm\n\n\nExample\n: deploy a Concourse Helm release onto an existing Kubernetes cluster\n\n\nHelm CLI\n\n\n\n\n\n\nPush the locally-developed Helm chart to a remote chart repository (ChartMuseum) and update our local repository index:\n\ncd\n charts/concourse \n\\\n\n        \n bash push.sh \n\\\n\n        \n helm repo update\n\n\n\n\n\n\n\nCreate the Kubernetes secrets necessary for the release:\n\nkubectl create secret generic concourse-concourse \n\\\n\n        --from-file\n=\nlive/prod/kube/ci/concourse/secrets/\n\n\n\n\n\n\n\nDeploy the Helm release: pull the chart, combine with our release values and submit to the Kubernetes API:\n\nhelm install \n\\\n\n        --name concourse \n\\\n\n        -f values.yaml \n\\\n\n        private/concourse\n\n\n\n\n\n\n\nUpgrade the release:\n\nhelm upgrade \n\\\n\n        -f values.yaml \n\\\n\n        concourse \n\\\n\n        private/concourse\n\n\n\n\n\n\n\nDestroy the release:\n\nhelm delete \n\\\n\n        concourse \n\\\n\n        --purge\n\n\n\n\n\n\n\nterraform-provider-helm (via Exekube)\n\n\n\n\n\n\nImport the \nhelm-release\n Terraform module and declare its dependencies:\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    source\n \n=\n \n/exekube/modules//helm-release\n\n  \n}\n\n\n  \ndependencies\n \n{\n\n\n    paths\n \n=\n \n[\n\n      \n../../../infra/gcp-gke\n,\n\n      \n../../core/ingress-controller\n,\n\n      \n../../core/kube-lego\n,\n\n      \n../chartmuseum\n,\n\n    \n]\n\n  \n}\n\n\n\n  include\n \n=\n \n{\n\n\n    path\n \n=\n \n${find_in_parent_folders()}\n\n  \n}\n\n\n}\n\n\n\n\n\n\n\n\nConfigure the release via the \nhelm-release\n module API:\n\nrelease_spec\n \n=\n \n{\n\n\n  enabled\n        \n=\n \ntrue\n\n\n  release_name\n   \n=\n \nconcourse\n\n\n  release_values\n \n=\n \nvalues.yaml\n\n\n\n  chart_repo\n    \n=\n \nprivate\n\n\n  chart_name\n    \n=\n \nconcourse\n\n\n  chart_version\n \n=\n \n1.0.0\n\n\n\n  domain_name\n \n=\n \nci.swarm.pw\n\n\n}\n\n\n\npre_hook\n \n=\n \n{\n\n\n  command\n \n=\n \n-EOF\n\n            \nkubectl\n \ncreate\n \nsecret\n \ngeneric\n \nconcourse-concourse\n \\\n\n            --from-file\n=\n/exekube/live/prod/kube/ci/concourse/secrets/\n \n||\n \ntrue\n \\\n            \n \ncd\n \n/exekube/charts/concourse/\n \\\n            \n \nbash\n \npush\n.\nsh\n \\\n            \n \nhelm\n \nrepo\n \nupdate\n\n            \nEOF\n\n\n}\n\n\n\n\n\n\n\n\nDeploy or upgrade the release:\n\nxk up live/prod/kube/ci/concourse\n\n\n\n\n\n\n\nDestroy the release:\n\nxk down live/prod/kube/ci/concourse", 
            "title": "Compare using Helm CLI and terraform-provider-helm"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#compare-deploying-helm-releases-via-helm-cli-to-using-terraform-provider-helm", 
            "text": "Example : deploy a Concourse Helm release onto an existing Kubernetes cluster", 
            "title": "Compare deploying Helm releases via Helm CLI to using terraform-provider-helm"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#helm-cli", 
            "text": "Push the locally-developed Helm chart to a remote chart repository (ChartMuseum) and update our local repository index: cd  charts/concourse  \\ \n          bash push.sh  \\ \n          helm repo update    Create the Kubernetes secrets necessary for the release: kubectl create secret generic concourse-concourse  \\ \n        --from-file = live/prod/kube/ci/concourse/secrets/    Deploy the Helm release: pull the chart, combine with our release values and submit to the Kubernetes API: helm install  \\ \n        --name concourse  \\ \n        -f values.yaml  \\ \n        private/concourse    Upgrade the release: helm upgrade  \\ \n        -f values.yaml  \\ \n        concourse  \\ \n        private/concourse    Destroy the release: helm delete  \\ \n        concourse  \\ \n        --purge", 
            "title": "Helm CLI"
        }, 
        {
            "location": "/misc/helm-cli-vs-terraform-provider-helm/#terraform-provider-helm-via-exekube", 
            "text": "Import the  helm-release  Terraform module and declare its dependencies: terragrunt   =   { \n   terraform   {      source   =   /exekube/modules//helm-release \n   } \n\n   dependencies   {      paths   =   [ \n       ../../../infra/gcp-gke , \n       ../../core/ingress-controller , \n       ../../core/kube-lego , \n       ../chartmuseum , \n     ] \n   }    include   =   {      path   =   ${find_in_parent_folders()} \n   }  }     Configure the release via the  helm-release  module API: release_spec   =   {    enabled          =   true    release_name     =   concourse    release_values   =   values.yaml    chart_repo      =   private    chart_name      =   concourse    chart_version   =   1.0.0    domain_name   =   ci.swarm.pw  }  pre_hook   =   {    command   =   -EOF \n             kubectl   create   secret   generic   concourse-concourse  \\             --from-file = /exekube/live/prod/kube/ci/concourse/secrets/   ||   true  \\\n               cd   /exekube/charts/concourse/  \\\n               bash   push . sh  \\\n               helm   repo   update \n             EOF  }     Deploy or upgrade the release: xk up live/prod/kube/ci/concourse    Destroy the release: xk down live/prod/kube/ci/concourse", 
            "title": "terraform-provider-helm (via Exekube)"
        }, 
        {
            "location": "/misc/configure-helm-release/", 
            "text": "Rails app live module example\n\n\nhttps://github.com/ilyasotkov/exekube/tree/feature/vault/live/prod/kube/apps/rails-app\n\n\nHere is a quick example of how you'd configure a Rails application Helm release using Exekube (this is a part of a of a \n\"live\" Terraform module\n, expressed in HashiCorp Configuration Language (HCL):\n\n\ncd\n live/prod/kube/apps/rails-app/\ntree .\n.\n\u251c\u2500\u2500 inputs.tfvars\n\u251c\u2500\u2500 terraform.tfvars\n\u2514\u2500\u2500 values.yaml\n\n\n\n\n# cat terraform.tfvars\n\n\n\nterragrunt\n \n=\n \n{\n\n  \nterraform\n \n{\n\n\n    source\n \n=\n \n/exekube/modules//helm-release\n\n  \n}\n\n\n  \ndependencies\n \n{\n\n\n    paths\n \n=\n \n[\n\n      \n../../../infra/gcp-gke\n,\n\n      \n../../core/ingress-controller\n,\n\n      \n../../core/kube-lego\n,\n\n      \n../../ci/chartmuseum\n,\n\n      \n../../ci/docker-registry\n,\n\n    \n]\n\n  \n}\n\n\n\n  include\n \n=\n \n{\n\n\n    path\n \n=\n \n${find_in_parent_folders()}\n\n  \n}\n\n\n}\n\n\n\n\n\n# cat inputs.tfvars\n\n\n\nrelease_spec\n \n=\n \n{\n\n\n  enabled\n        \n=\n \ntrue\n\n\n  domain_name\n    \n=\n \nrails-app.swarm.pw\n\n\n\n  release_name\n   \n=\n \nrails-app\n\n\n  release_values\n \n=\n \nvalues.yaml\n\n\n\n  chart_repo\n    \n=\n \nprivate\n\n\n  chart_name\n    \n=\n \nrails-app\n\n\n  chart_version\n \n=\n \n0.1.1\n\n\n}\n\n\n\n\n\n# cat values.yaml\n\n\n\nreplicaCount\n:\n \n2\n\n\nimage\n:\n\n  \nrepository\n:\n \nilyasotkov/rails-react-boilerplate\n\n  \ntag\n:\n \n0.1.0\n\n  \npullPolicy\n:\n \nAlways\n\n\ningress\n:\n\n  \nenabled\n:\n \ntrue\n\n  \nannotations\n:\n\n    \nkubernetes.io/ingress.class\n:\n \nnginx\n\n    \nkubernetes.io/tls-acme\n:\n \ntrue\n\n  \nhosts\n:\n\n    \n-\n \n${domain_name}\n\n  \ntls\n:\n\n    \n-\n \nsecretName\n:\n \n${domain_name}-tls\n\n      \nhosts\n:\n\n        \n-\n \n${domain_name}\n\n\npostgresql\n:\n\n  \npersistence\n:\n\n    \nenabled\n:\n \ntrue\n\n  \npostgresUser\n:\n \npostgres\n\n  \npostgresPassword\n:\n \npostgres", 
            "title": "Configure a Helm release"
        }, 
        {
            "location": "/misc/configure-helm-release/#rails-app-live-module-example", 
            "text": "https://github.com/ilyasotkov/exekube/tree/feature/vault/live/prod/kube/apps/rails-app  Here is a quick example of how you'd configure a Rails application Helm release using Exekube (this is a part of a of a  \"live\" Terraform module , expressed in HashiCorp Configuration Language (HCL):  cd  live/prod/kube/apps/rails-app/\ntree .\n.\n\u251c\u2500\u2500 inputs.tfvars\n\u251c\u2500\u2500 terraform.tfvars\n\u2514\u2500\u2500 values.yaml  # cat terraform.tfvars  terragrunt   =   { \n   terraform   {      source   =   /exekube/modules//helm-release \n   } \n\n   dependencies   {      paths   =   [ \n       ../../../infra/gcp-gke , \n       ../../core/ingress-controller , \n       ../../core/kube-lego , \n       ../../ci/chartmuseum , \n       ../../ci/docker-registry , \n     ] \n   }    include   =   {      path   =   ${find_in_parent_folders()} \n   }  }   # cat inputs.tfvars  release_spec   =   {    enabled          =   true    domain_name      =   rails-app.swarm.pw    release_name     =   rails-app    release_values   =   values.yaml    chart_repo      =   private    chart_name      =   rails-app    chart_version   =   0.1.1  }   # cat values.yaml  replicaCount :   2  image : \n   repository :   ilyasotkov/rails-react-boilerplate \n   tag :   0.1.0 \n   pullPolicy :   Always  ingress : \n   enabled :   true \n   annotations : \n     kubernetes.io/ingress.class :   nginx \n     kubernetes.io/tls-acme :   true \n   hosts : \n     -   ${domain_name} \n   tls : \n     -   secretName :   ${domain_name}-tls \n       hosts : \n         -   ${domain_name}  postgresql : \n   persistence : \n     enabled :   true \n   postgresUser :   postgres \n   postgresPassword :   postgres", 
            "title": "Rails app live module example"
        }, 
        {
            "location": "/misc/vault-integration/", 
            "text": "Vault on Kubernetes\n\n\n\n\nMissing\n\n\nSupport and integration for HashiCorp Vault has not been yet added in Exekube version 0.1.0.\n\n\n\n\nTest access to Vault from local machine\n\n\nxk kubectl port-forward \nvault-pod-name\n \n443\n:8200\n\ndocker ps\n\n\n\n\nUse HTTP (cURL)\n\n\nhttps://www.vaultproject.io/api/\n\n\ndocker \nexec\n \nlocal-container-id\n curl -k -vv https://localhost/v1/sys/seal-status/\n\n\n# https://www.vaultproject.io/api/system/init.html\n\ndocker \nexec\n \nlocal-container-id\n curl --request PUT -s -k --data \n{\nsecret_shares\n: 5, \nsecret_threshold\n: 3}\n https://localhost/v1/sys/init\n\n\n\n\nUse Vault CLI\n\n\ndocker \nexec\n -it \nlocal-container-id\n bash\n\nbash-4.3# vault init\n\n\n\n\nTest access to Vault from a cluster pod\n\n\nxk kubectl run my-shell --rm -i --tty --image ubuntu -- bash\n\napt-get update\napt-get install curl\ncurl -k --request PUT --data \n{\nsecret_shares\n: 5, \nsecret_threshold\n: 3}\n https://vault-vault:8200/v1/sys/init\n\n\n\n\nNotes and links\n\n\nExample implementation by CoreOS Tectonic\n\n\nhttps://coreos.com/tectonic/docs/latest/account/create-account.html\n\n\nKubernetes Auth Backend\n\n\nhttps://www.hashicorp.com/blog/hashicorp-vault-0-8-3\n\n\n\n\ntl;dr; Every Kubernetes pod gets a Service Account token that is automatically mounted at /var/run/secrets/kubernetes.io/serviceaccounts/token Now, you can use that token (JWT token) to also log into vault, if you enable the Kubernetes auth module and configure a Vault role for your Kubernetes service account.\n\n\nVault 0.8.3 introduces native Kubernetes auth backend that allows Kubernetes pods to directly receive and use Vault auth tokens without additional integration components.\n\n\n\n\nPrior to 0.8.3, a user accessing Vault via a pod required significant preparation work using an init pod or other custom interface. With the release of the Kubernetes auth backend, Vault now provides a production-ready interface for Kubernetes that allows a pod to authenticate with Vault via a JWT token from a pod\u2019s service account.\n\n\nView the documentation for more information on the Kubernetes auth backend.\n\n\nFor more information on the collaboration between Google and HashiCorp Vault, check out \u201cSecret and infrastructure management made easy with HashiCorp and Google Cloud\u201d and \u201cAuthenticating to Hashicorp Vault using GCE Signed Metadata\u201d published by Google.", 
            "title": "Use HashiCorp Vault to manage secrets"
        }, 
        {
            "location": "/misc/vault-integration/#vault-on-kubernetes", 
            "text": "Missing  Support and integration for HashiCorp Vault has not been yet added in Exekube version 0.1.0.", 
            "title": "Vault on Kubernetes"
        }, 
        {
            "location": "/misc/vault-integration/#test-access-to-vault-from-local-machine", 
            "text": "xk kubectl port-forward  vault-pod-name   443 :8200\n\ndocker ps", 
            "title": "Test access to Vault from local machine"
        }, 
        {
            "location": "/misc/vault-integration/#use-http-curl", 
            "text": "https://www.vaultproject.io/api/  docker  exec   local-container-id  curl -k -vv https://localhost/v1/sys/seal-status/ # https://www.vaultproject.io/api/system/init.html \ndocker  exec   local-container-id  curl --request PUT -s -k --data  { secret_shares : 5,  secret_threshold : 3}  https://localhost/v1/sys/init", 
            "title": "Use HTTP (cURL)"
        }, 
        {
            "location": "/misc/vault-integration/#use-vault-cli", 
            "text": "docker  exec  -it  local-container-id  bash\n\nbash-4.3# vault init", 
            "title": "Use Vault CLI"
        }, 
        {
            "location": "/misc/vault-integration/#test-access-to-vault-from-a-cluster-pod", 
            "text": "xk kubectl run my-shell --rm -i --tty --image ubuntu -- bash\n\napt-get update\napt-get install curl\ncurl -k --request PUT --data  { secret_shares : 5,  secret_threshold : 3}  https://vault-vault:8200/v1/sys/init", 
            "title": "Test access to Vault from a cluster pod"
        }, 
        {
            "location": "/misc/vault-integration/#notes-and-links", 
            "text": "", 
            "title": "Notes and links"
        }, 
        {
            "location": "/misc/vault-integration/#example-implementation-by-coreos-tectonic", 
            "text": "https://coreos.com/tectonic/docs/latest/account/create-account.html", 
            "title": "Example implementation by CoreOS Tectonic"
        }, 
        {
            "location": "/misc/vault-integration/#kubernetes-auth-backend", 
            "text": "https://www.hashicorp.com/blog/hashicorp-vault-0-8-3   tl;dr; Every Kubernetes pod gets a Service Account token that is automatically mounted at /var/run/secrets/kubernetes.io/serviceaccounts/token Now, you can use that token (JWT token) to also log into vault, if you enable the Kubernetes auth module and configure a Vault role for your Kubernetes service account.  Vault 0.8.3 introduces native Kubernetes auth backend that allows Kubernetes pods to directly receive and use Vault auth tokens without additional integration components.   Prior to 0.8.3, a user accessing Vault via a pod required significant preparation work using an init pod or other custom interface. With the release of the Kubernetes auth backend, Vault now provides a production-ready interface for Kubernetes that allows a pod to authenticate with Vault via a JWT token from a pod\u2019s service account.  View the documentation for more information on the Kubernetes auth backend.  For more information on the collaboration between Google and HashiCorp Vault, check out \u201cSecret and infrastructure management made easy with HashiCorp and Google Cloud\u201d and \u201cAuthenticating to Hashicorp Vault using GCE Signed Metadata\u201d published by Google.", 
            "title": "Kubernetes Auth Backend"
        }, 
        {
            "location": "/misc/feature-tracker/", 
            "text": "Feature tracker\n\n\n\n\nWarning\n\n\nThis section might be outdated.\n\n\n\n\nFeatures are marked with \u2714\ufe0f when they enter the \nalpha stage\n, meaning a minimum viable solution has been implemented\n\n\nCloud provider and local environment setup\n\n\n\n\n Create GCP account, enable billing in GCP Console (web GUI)\n\n\n Get credentials for GCP (\ncredentials.json\n)\n\n\n Authenticate to GCP using \ncredentials.json\n (for \ngcloud\n and \nterraform\n use)\n\n\n Enable terraform remote state in a Cloud Storage bucket\n\n\n\n\nCloud provider config\n\n\n\n\n Create GCP Folders and Projects and associated policies\n\n\n Create GCP IAM Service Accounts and IAM Policies for the Project\n\n\n\n\nCluster creation\n\n\n\n\n Create the GKE cluster\n\n\n Get cluster credentials (\n/root/.kube/config\n file)\n\n\n Initialize Helm\n\n\n\n\nCluster access control\n\n\n\n\n Add cluster namespaces (virtual clusters)\n\n\n Add cluster roles and role bindings\n\n\n Add cluster network policies\n\n\n\n\nSupporting tools\n\n\n\n\n Install cluster ingress controller (cloud load balancer)\n\n\n Install TLS certificates controller (kube-lego)\n\n\n Install Continuous Delivery tools\n\n\n Continuous delivery service (Drone / Jenkins)\n\n\n Helm chart repository (ChartMuseum)\n\n\n Private Docker registry\n\n\n Git service (Gitlab / Gogs)\n\n\n\n\n\n\n Monitoring and alerting tools (Prometheus / Grafana)\n\n\n\n\nUser apps and services\n\n\n\n\n Install \"hello-world\" apps like static sites, Ruby on Rails apps, etc.", 
            "title": "Project feature tracker"
        }, 
        {
            "location": "/misc/feature-tracker/#feature-tracker", 
            "text": "Warning  This section might be outdated.   Features are marked with \u2714\ufe0f when they enter the  alpha stage , meaning a minimum viable solution has been implemented", 
            "title": "Feature tracker"
        }, 
        {
            "location": "/misc/feature-tracker/#cloud-provider-and-local-environment-setup", 
            "text": "Create GCP account, enable billing in GCP Console (web GUI)   Get credentials for GCP ( credentials.json )   Authenticate to GCP using  credentials.json  (for  gcloud  and  terraform  use)   Enable terraform remote state in a Cloud Storage bucket", 
            "title": "Cloud provider and local environment setup"
        }, 
        {
            "location": "/misc/feature-tracker/#cloud-provider-config", 
            "text": "Create GCP Folders and Projects and associated policies   Create GCP IAM Service Accounts and IAM Policies for the Project", 
            "title": "Cloud provider config"
        }, 
        {
            "location": "/misc/feature-tracker/#cluster-creation", 
            "text": "Create the GKE cluster   Get cluster credentials ( /root/.kube/config  file)   Initialize Helm", 
            "title": "Cluster creation"
        }, 
        {
            "location": "/misc/feature-tracker/#cluster-access-control", 
            "text": "Add cluster namespaces (virtual clusters)   Add cluster roles and role bindings   Add cluster network policies", 
            "title": "Cluster access control"
        }, 
        {
            "location": "/misc/feature-tracker/#supporting-tools", 
            "text": "Install cluster ingress controller (cloud load balancer)   Install TLS certificates controller (kube-lego)   Install Continuous Delivery tools   Continuous delivery service (Drone / Jenkins)   Helm chart repository (ChartMuseum)   Private Docker registry   Git service (Gitlab / Gogs)     Monitoring and alerting tools (Prometheus / Grafana)", 
            "title": "Supporting tools"
        }, 
        {
            "location": "/misc/feature-tracker/#user-apps-and-services", 
            "text": "Install \"hello-world\" apps like static sites, Ruby on Rails apps, etc.", 
            "title": "User apps and services"
        }, 
        {
            "location": "/misc/secrets/", 
            "text": "Managing secrets\n\n\n\n\nMissing\n\n\nThis section has not been written yet. Want to help? \nSubmit a pull request\n.", 
            "title": "Managing secrets in Exekube"
        }, 
        {
            "location": "/misc/secrets/#managing-secrets", 
            "text": "Missing  This section has not been written yet. Want to help?  Submit a pull request .", 
            "title": "Managing secrets"
        }, 
        {
            "location": "/reference/gke-cluster/", 
            "text": "gke-cluster module\n\n\nModule inputs and defaults:\n\n\nvariable\n \ngcp_zone\n \n{\n\n\n  default\n \n=\n \neurope-west1-d\n\n\n}\n\n\n\nvariable\n \ngcp_region\n \n{\n\n\n  default\n \n=\n \neurope-west1\n\n\n}\n\n\n\nvariable\n \ngcp_project\n \n{}\n\n\n\n# ------------------------------------------------------------------------------\n\n\n# Cluster vars\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \ncluster_name\n \n{\n\n\n  default\n \n=\n \nk8s-cluster\n\n\n}\n\n\n\nvariable\n \nnode_type\n \n{\n\n\n  default\n \n=\n \nn1-standard-2\n\n\n}\n\n\n\nvariable\n \ngke_version\n \n{\n\n\n  default\n \n=\n \n1.8.7-gke.1\n\n\n}\n\n\n\nvariable\n \nenable_legacy_auth\n \n{\n\n\n  default\n \n=\n \nfalse\n\n\n}\n\n\n\n# ------------------------------------------------------------------------------\n\n\n# Node pool vars\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \nnodepool_name\n \n{\n\n\n  default\n \n=\n \nk8s-nodepool\n\n\n}\n\n\n\nvariable\n \nnodepool_max_nodes\n \n{\n\n\n  default\n \n=\n \n4\n\n\n}\n\n\n\nvariable\n \nnodepool_machine_type\n \n{\n\n\n  default\n \n=\n \nn1-standard-2\n\n\n}", 
            "title": "gke-cluster module"
        }, 
        {
            "location": "/reference/gke-cluster/#gke-cluster-module", 
            "text": "Module inputs and defaults:  variable   gcp_zone   {    default   =   europe-west1-d  }  variable   gcp_region   {    default   =   europe-west1  }  variable   gcp_project   {}  # ------------------------------------------------------------------------------  # Cluster vars  # ------------------------------------------------------------------------------  variable   cluster_name   {    default   =   k8s-cluster  }  variable   node_type   {    default   =   n1-standard-2  }  variable   gke_version   {    default   =   1.8.7-gke.1  }  variable   enable_legacy_auth   {    default   =   false  }  # ------------------------------------------------------------------------------  # Node pool vars  # ------------------------------------------------------------------------------  variable   nodepool_name   {    default   =   k8s-nodepool  }  variable   nodepool_max_nodes   {    default   =   4  }  variable   nodepool_machine_type   {    default   =   n1-standard-2  }", 
            "title": "gke-cluster module"
        }, 
        {
            "location": "/reference/helm-release/", 
            "text": "helm-release module reference\n\n\nModule inputs and defaults:\n\n\n# ------------------------------------------------------------------------------\n\n\n# Pre-hook and post-hook, to be run before creation and after release creation\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \npre_hook\n \n{\n\n\n  type\n \n=\n \nmap\n\n\n\n  default\n \n=\n \n{\n\n\n    command\n \n=\n \necho hello from pre_hook\n\n  \n}\n\n\n}\n\n\n\nvariable\n \npost_hook\n \n{\n\n\n  type\n \n=\n \nmap\n\n\n\n  default\n \n=\n \n{\n\n\n    command\n \n=\n \necho hello from post_hook\n\n  \n}\n\n\n}\n\n\n\n# ------------------------------------------------------------------------------\n\n\n# Helm release input variables\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \nrelease_spec\n \n{\n\n\n  type\n \n=\n \nmap\n\n\n\n  default\n \n=\n \n{\n\n\n    enabled\n        \n=\n \nfalse\n\n\n    chart_repo\n     \n=\n \n\n\n    chart_name\n     \n=\n \n\n\n    chart_version\n  \n=\n \n\n\n    release_name\n   \n=\n \n\n\n    release_values\n \n=\n \nvalues.yaml\n\n\n\n    domain_name\n \n=\n \n\n  \n}\n\n\n}\n\n\n\n# ------------------------------------------------------------------------------\n\n\n# Kubernetes secret inputs\n\n\n# ------------------------------------------------------------------------------\n\n\n\nvariable\n \nxk_live_dir\n \n{}\n\n\n\nvariable\n \ningress_basic_auth\n \n{\n\n\n  type\n \n=\n \nmap\n\n\n\n  default\n \n=\n \n{\n\n\n    username\n    \n=\n \n\n\n    password\n    \n=\n \n\n\n    secret_name\n \n=\n \n\n  \n}\n\n\n}", 
            "title": "helm-release module"
        }, 
        {
            "location": "/reference/helm-release/#helm-release-module-reference", 
            "text": "Module inputs and defaults:  # ------------------------------------------------------------------------------  # Pre-hook and post-hook, to be run before creation and after release creation  # ------------------------------------------------------------------------------  variable   pre_hook   {    type   =   map    default   =   {      command   =   echo hello from pre_hook \n   }  }  variable   post_hook   {    type   =   map    default   =   {      command   =   echo hello from post_hook \n   }  }  # ------------------------------------------------------------------------------  # Helm release input variables  # ------------------------------------------------------------------------------  variable   release_spec   {    type   =   map    default   =   {      enabled          =   false      chart_repo       =        chart_name       =        chart_version    =        release_name     =        release_values   =   values.yaml      domain_name   =   \n   }  }  # ------------------------------------------------------------------------------  # Kubernetes secret inputs  # ------------------------------------------------------------------------------  variable   xk_live_dir   {}  variable   ingress_basic_auth   {    type   =   map    default   =   {      username      =        password      =        secret_name   =   \n   }  }", 
            "title": "helm-release module reference"
        }
    ]
}